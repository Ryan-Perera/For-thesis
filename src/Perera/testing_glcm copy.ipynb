{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will test which comnination of GLCM features provide the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from skimage.feature import graycoprops, graycomatrix, local_binary_pattern, hog\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import data, exposure, io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updated json file path\n",
    "updated_json_file = r\"C:\\Users\\mdrsp\\Desktop\\School\\For thesis\\spv_images\\InfraredSolarModules\\module_metadata_update.json\"\n",
    "\n",
    "# images folder path\n",
    "path_2_images = r\"C:\\Users\\mdrsp\\Desktop\\School\\For thesis\\spv_images\\InfraredSolarModules\"\n",
    "\n",
    "# path to any image\n",
    "sample_img = r\"C:\\Users\\mdrsp\\Desktop\\School\\For thesis\\spv_images\\InfraredSolarModules\\images\\0.jpg\"\n",
    "\n",
    "def path2img(path):\n",
    "    img = io.imread(path,as_gray=True)\n",
    "    return img\n",
    "\n",
    "img_test = path2img(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local binary Pattern\n",
    "def lbp_features(img):\n",
    "    from skimage.feature import local_binary_pattern\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp_value = local_binary_pattern(img, n_points,radius)\n",
    "    return lbp_value\n",
    "\n",
    "\n",
    "# Histogram of oriented gradients\n",
    "def hog_features(img):\n",
    "    # Calculate the HOG features\n",
    "    fd, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16),\n",
    "                        cells_per_block=(1, 1), visualize=True)\n",
    "\n",
    "    return fd\n",
    "\n",
    "def glcm_features(img):\n",
    "    features = ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'correlation', 'energy']\n",
    "    glcm = graycomatrix(img, distances=[1,2,3], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], levels=256, symmetric=True, normed=True)\n",
    "    feature_vector = []\n",
    "    for feature in features:\n",
    "        feature_value = graycoprops(glcm, feature)\n",
    "        feature_vector.append(feature_value)\n",
    "    return np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[4.71097826e+01, 8.17179487e+01, 4.38974359e+01, 8.14894091e+01],\n",
       "        [1.55646591e+02, 8.17179487e+01, 1.16151316e+02, 8.14894091e+01],\n",
       "        [2.68089286e+02, 2.31035885e+02, 1.81272523e+02, 2.30721292e+02]],\n",
       "\n",
       "       [[4.61847826e+00, 6.57859532e+00, 4.69230769e+00, 6.32998885e+00],\n",
       "        [8.30113636e+00, 6.57859532e+00, 8.10745614e+00, 6.32998885e+00],\n",
       "        [1.06988095e+01, 1.12559809e+01, 1.05990991e+01, 1.09007177e+01]],\n",
       "\n",
       "       [[2.39705537e-01, 1.47954025e-01, 2.03650099e-01, 1.62216918e-01],\n",
       "        [1.39733351e-01, 1.47954025e-01, 1.21477118e-01, 1.62216918e-01],\n",
       "        [1.28447440e-01, 9.73752777e-02, 8.50137335e-02, 1.02454777e-01]],\n",
       "\n",
       "       [[1.63220699e-03, 1.32735279e-03, 1.55747772e-03, 1.32300285e-03],\n",
       "        [1.30488120e-03, 1.32735279e-03, 1.15239785e-03, 1.32300285e-03],\n",
       "        [1.29109977e-03, 1.08099059e-03, 1.08617908e-03, 1.15396282e-03]],\n",
       "\n",
       "       [[9.53694801e-01, 9.15020146e-01, 9.57824915e-01, 9.15239210e-01],\n",
       "        [8.38955348e-01, 9.15020146e-01, 8.83696362e-01, 9.15239210e-01],\n",
       "        [7.20720397e-01, 7.32336679e-01, 8.12544451e-01, 7.33302142e-01]],\n",
       "\n",
       "       [[4.04005816e-02, 3.64328532e-02, 3.94648922e-02, 3.63731061e-02],\n",
       "        [3.61231394e-02, 3.64328532e-02, 3.39469859e-02, 3.63731061e-02],\n",
       "        [3.59318768e-02, 3.28784214e-02, 3.29572311e-02, 3.39700283e-02]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glcm_features(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save teh GLCM values of the 20k images into arrays\n",
    "file = open(updated_json_file)\n",
    "data = json.load(file)\n",
    "\n",
    "anomaly_list = []\n",
    "feature_list = []\n",
    "\n",
    "os.chdir(path_2_images)\n",
    "for key,value in data.items():\n",
    "    current_img_path = value['image_filepath']\n",
    "    anomaly = value['anomaly_class']\n",
    "\n",
    "    img = path2img(current_img_path)\n",
    "     \n",
    "    feature_list.append(glcm_features(img))\n",
    "    feature_list.append(list(lbp_features(img)))\n",
    "    feature_list.append(list(hog_features(img)))\n",
    "\n",
    "\n",
    "    anomaly_list.append(anomaly) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(values,features= ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'correlation', 'energy', 'LBP', 'HOG']):\n",
    "    new_values = []\n",
    "    for row in values:\n",
    "        temp = []\n",
    "        temp.append(row[0]) if 'contrast' in features else None\n",
    "        temp.append(row[1]) if 'dissimilarity' in features else None\n",
    "        temp.append(row[2]) if 'homogeneity' in features else None\n",
    "        temp.append(row[3]) if 'ASM' in features else None\n",
    "        temp.append(row[4]) if 'correlation' in features else None\n",
    "        temp.append(row[5]) if 'energy' in features else None\n",
    "\n",
    "        temp.append(row[6]) if 'LBP' in features else None\n",
    "        temp.append(row[7]) if 'HOG' in features else None\n",
    "\n",
    "        temp = np.ravel(temp)\n",
    "\n",
    "        \n",
    "        new_values.append(temp)\n",
    "\n",
    "    return np.array(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdrsp\\AppData\\Local\\Temp\\ipykernel_8072\\1612632229.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(new_values)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m feature_vector \u001b[39m=\u001b[39m get_features(feature_list, \u001b[39mlist\u001b[39m(combination))\n\u001b[0;32m      7\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 8\u001b[0m images \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(feature_vector)\n\u001b[0;32m      9\u001b[0m images \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(images)\n\u001b[0;32m     10\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(anomaly_list)\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:848\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    847\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    851\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    862\u001b[0m     X,\n\u001b[0;32m    863\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    864\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    865\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    866\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mdrsp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "features = ['contrast', 'dissimilarity', 'homogeneity', 'ASM', 'correlation', 'energy']\n",
    "best_accuracy = 0\n",
    "best_combination = []\n",
    "for i in range(3,len(features)+1):\n",
    "    for combination in combinations(features,i):\n",
    "        feature_vector = get_features(feature_list, list(combination))\n",
    "        scaler = StandardScaler()\n",
    "        images = scaler.fit_transform(feature_vector)\n",
    "        images = np.array(images)\n",
    "        labels = np.array(anomaly_list)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "        clf = SVC(kernel='poly', C=1, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "        print(f'Accuracy: {accuracy:.3f}, {list(combination)}')\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_combination = list(combination)\n",
    "\n",
    "print(\"- - output - -\")\n",
    "print(f'Higest accuracy: {best_accuracy},{best_combination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
